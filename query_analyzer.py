# -*- coding: utf-8 -*-
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è B2B –∫–∞–º–ø–∞–Ω–∏–π –Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç
–ó–∞–º–µ–Ω–∞ bid_manager.py ‚Äî –≤–º–µ—Å—Ç–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å—Ç–∞–≤–∫–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç
–ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞.

–§—É–Ω–∫—Ü–∏–∏:
1. –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –ø–æ –ø–æ–∏—Å–∫–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º (SEARCH_QUERY_PERFORMANCE_REPORT)
2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤: B2B-—Ü–µ–ª–µ–≤–æ–π / –º—É—Å–æ—Ä / –ø–æ–¥ –≤–æ–ø—Ä–æ—Å–æ–º
3. –ê–≤—Ç–æ–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–∏–Ω—É—Å-—Å–ª–æ–≤ –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –∫–∞–º–ø–∞–Ω–∏–∏
4. –û—Ç—á—ë—Ç –≤ Telegram
"""

import re
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from pathlib import Path
from dataclasses import dataclass, field

from yandex_direct_api import YandexDirectAPI


# ‚îÄ‚îÄ‚îÄ –ö–∞–º–ø–∞–Ω–∏–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
B2B_CAMPAIGN_IDS = [705839254, 705839266]  # Hot + Geo

# ‚îÄ‚îÄ‚îÄ –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# –û–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –º—É—Å–æ—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã ‚Äî —Å—Ä–∞–∑—É –≤ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞
JUNK_PATTERNS = [
    # –¢—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    r"\b(–≤–∞–∫–∞–Ω—Å–∏—è|–≤–∞–∫–∞–Ω—Å–∏–∏|–∑–∞—Ä–ø–ª–∞—Ç–∞|—Ä–∞–±–æ—Ç–∞\b|–ø–æ–¥—Ä–∞–±–æ—Ç–∫–∞|—É—Å—Ç—Ä–æ–∏—Ç—å—Å—è|—Ä–µ–∑—é–º–µ|"
    r"—Ç—Ä–µ–±—É–µ—Ç—Å—è|—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞|hh\.ru|headhunter|—Ç—Ä—É–¥–æ—É—Å—Ç—Ä)",
    # –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã / –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã
    r"\b(blablacar|–±–ª–∞–±–ª–∞–∫–∞—Ä|gett|—Å–∏—Ç–∏–º–æ–±–∏–ª|—è–Ω–¥–µ–∫—Å\s*—Ç–∞–∫—Å–∏|—è–Ω–¥–µ–∫—Å\s*go|"
    r"–¥–µ–ª–∏–º–æ–±–∏–ª—å|—è–Ω–¥–µ–∫—Å\s*–¥—Ä–∞–π–≤|kiwi|gettransfer|intui|wheely|–º–∞–∫—Å–∏–º\s+—Ç–∞–∫—Å–∏|"
    r"uber|—É–±–µ—Ä\b|–∏–Ω–¥—Ä–∞–π–≤–µ—Ä|indriver|bolt\b|didi)",
    # –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç (–ø–æ–µ–∑–¥\b —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å "–ø–æ–µ–∑–¥–∫–∞/–ø–æ–µ–∑–¥–æ–∫")
    r"\b(–∞–≤—Ç–æ–±—É—Å\w*|–ø–æ–µ–∑–¥\b|–ø–æ–µ–∑–¥–æ–≤\b|–ø–æ–µ–∑–¥–æ–º\b|—ç–ª–µ–∫—Ç—Ä–∏—á–∫\w*|–º–∞—Ä—à—Ä—É—Ç–∫\w*|"
    r"—ç–≤–∞–∫—É–∞—Ç–æ—Ä\w*|–∞–≤–∏–∞\w*|—Å–∞–º–æ–ª–µ—Ç\w*|–ø–∞—Ä–æ–º\b|–º–µ—Ç—Ä–æ\b|—Ç—Ä–æ–ª–ª–µ–π–±—É—Å|—Ç—Ä–∞–º–≤–∞–π|"
    r"—Ñ—É—Ä–∞\b|–≥–∞–∑–µ–ª—å\b|–∂\.?–¥\.?\b)",
    # –¢—É—Ä–∏–∑–º / –ª–∏—á–Ω–æ–µ
    r"\b(—Å–≤–∞–¥—å–±–∞|–ø–æ—Ö–æ—Ä–æ–Ω—ã|—Ä–æ–¥–¥–æ–º|–¥–µ—Ç—Å–∫–æ–µ\s*–∫—Ä–µ—Å–ª–æ|–≥–æ—Ä–Ω–æ–ª—ã–∂–∫–∞|—ç–∫—Å–∫—É—Ä—Å–∏—è|"
    r"—Ç—É—Ä–∏–∑–º|–æ—Ç–ø—É—Å–∫|—Ç—É—Ä–ø–æ–µ–∑–¥–∫–∞|–ø—É—Ç–µ–≤–∫–∞|—Å–∞–Ω–∞—Ç–æ—Ä–∏–π|–ø–ª—è–∂|–º–æ—Ä–µ\b)",
    # –î–µ—à–µ–≤–∏–∑–Ω–∞
    r"\b(–±–µ—Å–ø–ª–∞—Ç–Ω–æ|—Ö–∞–ª—è–≤–∞|–ø—Ä–æ–º–æ–∫–æ–¥|–∫—É–ø–æ–Ω|—Å–∫–∏–¥–∫–∞\s+\d+%)",
    # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è / –æ–±–∑–æ—Ä—ã
    r"\b(—Å–∫–∞—á–∞—Ç—å|–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ|–æ—Ç–∑—ã–≤—ã\b|—Ä–µ–π—Ç–∏–Ω–≥|—Ñ–æ—Ä—É–º|–≤–∏–∫–∏–ø–µ–¥–∏—è|youtube|—é—Ç—É–±)",
    # –û–±—É—á–µ–Ω–∏–µ
    r"\b(–æ–±—É—á–µ–Ω–∏–µ|–∫—É—Ä—Å—ã|–∞–≤—Ç–æ—à–∫–æ–ª–∞|–ª–µ–∫—Ü–∏—è|–¥–∏–ø–ª–æ–º\b)",
    # –ì—Ä—É–∑–æ–ø–µ—Ä–µ–≤–æ–∑–∫–∏
    r"\b(–≥—Ä—É–∑–æ–ø–µ—Ä–µ–≤–æ–∑|–≥—Ä—É–∑—á–∏–∫|–≥—Ä—É–∑–æ–≤–æ–π|–ø–µ—Ä–µ–µ–∑–¥|–º–µ–±–µ–ª|—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫)",
    # –†–∏—Ç—É–∞–ª—å–Ω—ã–µ / –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∞–≤—Ç–æ
    r"\b(–∫–∞—Ç–∞—Ñ–∞–ª–∫|—Ä–∏—Ç—É–∞–ª—å–Ω|—Å–∫–æ—Ä–∞—è\s*–ø–æ–º–æ—â—å)",
    # –ö–∞—Ä—à–µ—Ä–∏–Ω–≥ / –ø—Ä–æ–∫–∞—Ç
    r"\b(–∫–∞—Ä—à–µ—Ä–∏–Ω–≥|–ø—Ä–æ–∫–∞—Ç\s+–∞–≤—Ç–æ|–∞—Ä–µ–Ω–¥–∞\s+–∞–≤—Ç–æ|car\s*sharing)",
    # –ñ–∏–≤–æ—Ç–Ω—ã–µ
    r"\b(–∫–æ—à–∫–∞|–∫–æ—à–∫—É|—Å–æ–±–∞–∫–∞|—Å–æ–±–∞–∫—É|–∂–∏–≤–æ—Ç–Ω|–ø–∏—Ç–æ–º–µ—Ü)",
    # C2C-–º–∞—Ä–∫–µ—Ä—ã (–µ—Å–ª–∏ –ø–æ–ø–∞–ª–∏ –≤ B2B –∫–∞–º–ø–∞–Ω–∏—é)
    r"\b(–ø–æ–ø—É—Ç–∫–∞|–ø–æ–ø—É—Ç—á–∏–∫|–ø–æ–¥–≤–µ–∑—Ç–∏|–ø–æ–¥–≤–æ–∑\b)",
]

# B2B-–º–∞—Ä–∫–µ—Ä—ã ‚Äî –∑–∞–ø—Ä–æ—Å—ã —Å –Ω–∏–º–∏ —Ç–æ—á–Ω–æ —Ü–µ–ª–µ–≤—ã–µ
B2B_MARKERS = [
    r"\b(–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤|–¥–ª—è\s+–±–∏–∑–Ω–µ—Å|–¥–ª—è\s+–∫–æ–º–ø–∞–Ω|–¥–ª—è\s+–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü|–¥–ª—è\s+—é—Ä–ª–∏—Ü|"
    r"—é—Ä–∏–¥–∏—á–µ—Å–∫|—é—Ä–ª–∏—Ü–æ|–¥–ª—è\s+—Å–æ—Ç—Ä—É–¥–Ω–∏–∫)",
    r"\b(–¥–æ–≥–æ–≤–æ—Ä|–ø–æ\s+—Å—á–µ—Ç—É|–ø–æ\s+—Å—á—ë—Ç—É|–±–µ–∑–Ω–∞–ª|–ø–æ—Å—Ç–æ–ø–ª–∞—Ç–∞|–Ω–∞–ª\s+–±–µ–∑–Ω–∞–ª|"
    r"–∑–∞–∫—Ä—ã–≤–∞—é—â|–¥–æ–∫—É–º–µ–Ω—Ç|–∞–∫—Ç\b|—É–ø–¥\b|—ç–¥–æ\b|–±—É—Ö–≥–∞–ª—Ç–µ—Ä|—Å—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä|"
    r"–∞–≤–∞–Ω—Å–æ–≤|–æ—Ç—á–µ—Ç|—á–µ–∫\s+qr|—Ä–µ–µ—Å—Ç—Ä\s+–ø–æ–µ–∑–¥)",
    r"\b(–∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫|–¥–µ–ª–µ–≥–∞—Ü–∏|–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü|—Å–µ–º–∏–Ω–∞—Ä|—Ñ–æ—Ä—É–º|–º–µ—Ä–æ–ø—Ä–∏—è—Ç|–≤—ã—Å—Ç–∞–≤–∫)",
    r"\b(–≤–∞—Ö—Ç|–±—Ä–∏–≥–∞–¥|–º–æ–Ω—Ç–∞–∂–Ω–∏–∫|—Å—Ç—Ä–æ–∏—Ç–µ–ª|–ø–µ—Ä–µ–≤–æ–∑–∫–∞\s+—Ä–∞–±–æ—á–∏—Ö|"
    r"–¥–æ—Å—Ç–∞–≤–∫–∞\s+–±—Ä–∏–≥–∞–¥|—Ä–∞–∑–≤–æ–∑–∫–∞\s+—Å–æ—Ç—Ä—É–¥–Ω–∏–∫)",
    r"\b(–º–µ–¥–∏—Ü–∏–Ω—Å–∫|–º–µ–¥–ø–µ—Ä—Å–æ–Ω–∞–ª|–ø–∞—Ü–∏–µ–Ω—Ç|–∫–ª–∏–Ω–∏–∫|—Ñ–∞—Ä–º)",
    r"\b(—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä|–º–µ–∂–≥–æ—Ä–æ–¥|–º–µ–∂–¥—É–≥–æ—Ä–æ–¥–Ω)",
    r"\b(–Ω–¥—Å|–æ–ø–ª–∞—Ç–∞\s+–ø–æ\s+—Ä–µ–∫–≤–∏–∑–∏—Ç|—Ç–µ–Ω–¥–µ—Ä|–∑–∞–∫—É–ø–∫|–≥–æ—Å–∑–∞–∫—É–ø–∫)",
]

# –°–ª–æ–≤–∞-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –≤ –º–∏–Ω—É—Å, –µ—Å–ª–∏ –Ω–µ—Ç B2B-–º–∞—Ä–∫–µ—Ä–æ–≤ —Ä—è–¥–æ–º
SUSPICIOUS_PATTERNS = [
    r"\b(–¥–µ—à–µ–≤|–Ω–µ–¥–æ—Ä–æ–≥–æ|—ç–∫–æ–Ω–æ–º|–±—é–¥–∂–µ—Ç–Ω)",
    r"\b(–ª–∏–º—É–∑–∏–Ω|–≤–∏–ø|vip|–ø—Ä–µ–º–∏—É–º|–±–∏–∑–Ω–µ—Å.–∫–ª–∞—Å—Å)",
    r"\b(–∞—ç—Ä–æ–ø–æ—Ä—Ç|–≤–æ–∫–∑–∞–ª|–∂–¥\s*–≤–æ–∫–∑–∞–ª)",
    r"\b(—Ç–∞–∫—Å–∏\s+–º–æ—Å–∫–≤–∞|—Ç–∞–∫—Å–∏\s+—Å–ø–±)\b",  # —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ
]


@dataclass
class QueryAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    query: str
    impressions: int
    clicks: int
    cost: float
    ctr: float
    campaign_id: int
    classification: str  # "junk", "b2b", "suspicious", "unknown"
    matched_pattern: str = ""


@dataclass
class AnalysisReport:
    """–°–≤–æ–¥–Ω—ã–π –æ—Ç—á—ë—Ç"""
    period_days: int
    total_queries: int = 0
    total_cost: float = 0
    junk_queries: List[QueryAnalysis] = field(default_factory=list)
    b2b_queries: List[QueryAnalysis] = field(default_factory=list)
    suspicious_queries: List[QueryAnalysis] = field(default_factory=list)
    unknown_queries: List[QueryAnalysis] = field(default_factory=list)
    new_negatives_added: List[str] = field(default_factory=list)
    existing_negatives_skipped: int = 0


class QueryAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""

    HISTORY_FILE = Path(__file__).parent / "reports" / "query_history.json"

    def __init__(self):
        self.api = YandexDirectAPI()
        self._junk_re = [re.compile(p, re.IGNORECASE) for p in JUNK_PATTERNS]
        self._b2b_re = [re.compile(p, re.IGNORECASE) for p in B2B_MARKERS]
        self._suspicious_re = [re.compile(p, re.IGNORECASE) for p in SUSPICIOUS_PATTERNS]
        self._history = self._load_history()

    # ‚îÄ‚îÄ‚îÄ –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def get_search_queries(self, campaign_ids: List[int],
                           days: int = 7) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç—á—ë—Ç –ø–æ –ø–æ–∏—Å–∫–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º."""
        date_from = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
        date_to = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

        print(f"–ó–∞–ø—Ä–∞—à–∏–≤–∞—é –æ—Ç—á—ë—Ç: {date_from} ‚Äî {date_to}")

        report = self.api.get_report(
            report_type="SEARCH_QUERY_PERFORMANCE_REPORT",
            date_from=date_from,
            date_to=date_to,
            field_names=[
                "CampaignId", "Query", "Impressions", "Clicks",
                "Cost", "Ctr", "AvgCpc",
            ],
            campaign_ids=campaign_ids,
        )

        rows = []
        lines = report.strip().split("\n")
        if len(lines) < 2:
            return rows

        headers = lines[0].split("\t")
        for line in lines[1:]:
            vals = line.split("\t")
            if len(vals) != len(headers):
                continue
            row = dict(zip(headers, vals))
            rows.append(row)

        print(f"–ü–æ–ª—É—á–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(rows)}")
        return rows

    def get_current_negatives(self, campaign_ids: List[int]) -> set:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞ –∫–∞–º–ø–∞–Ω–∏–π."""
        import requests
        negatives = set()
        headers = {
            "Authorization": f"Bearer {self.api.token}",
            "Accept-Language": "ru",
        }
        r = requests.post(
            self.api.base_url + "campaigns",
            json={
                "method": "get",
                "params": {
                    "SelectionCriteria": {"Ids": campaign_ids},
                    "FieldNames": ["Id", "NegativeKeywords"],
                }
            },
            headers=headers,
        )
        data = r.json().get("result", {}).get("Campaigns", [])
        for c in data:
            nk = c.get("NegativeKeywords", {})
            if isinstance(nk, dict):
                for w in nk.get("Items", []):
                    negatives.add(w.lower().strip())
            elif isinstance(nk, list):
                for w in nk:
                    negatives.add(w.lower().strip())
        return negatives

    # ‚îÄ‚îÄ‚îÄ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def classify_query(self, query: str) -> Tuple[str, str]:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.

        Returns:
            (classification, matched_pattern)
        """
        q = query.lower().strip()

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º B2B-–º–∞—Ä–∫–µ—Ä—ã (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for pattern in self._b2b_re:
            m = pattern.search(q)
            if m:
                return ("b2b", m.group())

        # –ó–∞—Ç–µ–º –º—É—Å–æ—Ä–Ω—ã–µ
        for pattern in self._junk_re:
            m = pattern.search(q)
            if m:
                return ("junk", m.group())

        # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç B2B-–º–∞—Ä–∫–µ—Ä–æ–≤)
        for pattern in self._suspicious_re:
            m = pattern.search(q)
            if m:
                return ("suspicious", m.group())

        return ("unknown", "")

    # ‚îÄ‚îÄ‚îÄ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∏–Ω—É—Å-—Å–ª–æ–≤ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def extract_negative_word(self, query: str, matched: str) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ—á—å –º–∏–Ω—É—Å-—Å–ª–æ–≤–æ –∏–∑ –º—É—Å–æ—Ä–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ –º–∏–Ω—É—Å-—Å–ª–æ–≤–æ, –∞ –Ω–µ –≤–µ—Å—å –∑–∞–ø—Ä–æ—Å.
        """
        # –ë–µ—Ä—ë–º —Å–æ–≤–ø–∞–≤—à–µ–µ —Å–ª–æ–≤–æ –∫–∞–∫ –º–∏–Ω—É—Å
        word = matched.strip().lower()

        # –£–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
        if len(word) < 3:
            return None

        # –ï—Å–ª–∏ —ç—Ç–æ —Å–æ—Å—Ç–∞–≤–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ ‚Äî –±–µ—Ä—ë–º –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–ª–æ–≤–æ
        parts = word.split()
        if len(parts) > 2:
            return parts[0]  # –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ

        return word

    # ‚îÄ‚îÄ‚îÄ –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def analyze(self, campaign_ids: List[int] = None,
                days: int = 7,
                auto_add: bool = False) -> AnalysisReport:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.

        Args:
            campaign_ids: ID –∫–∞–º–ø–∞–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî B2B)
            days: –ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞
            auto_add: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è—Ç—å –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞

        Returns:
            AnalysisReport
        """
        if campaign_ids is None:
            campaign_ids = B2B_CAMPAIGN_IDS

        report = AnalysisReport(period_days=days)

        # 1. –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã
        queries = self.get_search_queries(campaign_ids, days)
        if not queries:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –ø–æ–∏—Å–∫–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º")
            return report

        # 2. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞
        existing_negatives = self.get_current_negatives(campaign_ids)
        print(f"–¢–µ–∫—É—â–∏—Ö –º–∏–Ω—É—Å-—Å–ª–æ–≤: {len(existing_negatives)}")

        # 3. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å
        new_negatives = set()

        for row in queries:
            try:
                query_text = row.get("Query", "")
                impressions = int(row.get("Impressions", 0))
                clicks = int(row.get("Clicks", 0))
                cost = float(row.get("Cost", 0))
                ctr = float(row.get("Ctr", 0))
                cid = int(row.get("CampaignId", 0))
            except (ValueError, TypeError):
                continue

            classification, matched = self.classify_query(query_text)

            qa = QueryAnalysis(
                query=query_text,
                impressions=impressions,
                clicks=clicks,
                cost=cost,
                ctr=ctr,
                campaign_id=cid,
                classification=classification,
                matched_pattern=matched,
            )

            report.total_queries += 1
            report.total_cost += cost

            if classification == "junk":
                report.junk_queries.append(qa)
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∏–Ω—É—Å-—Å–ª–æ–≤–æ
                neg = self.extract_negative_word(query_text, matched)
                if neg and neg not in existing_negatives:
                    new_negatives.add(neg)
                elif neg:
                    report.existing_negatives_skipped += 1

            elif classification == "b2b":
                report.b2b_queries.append(qa)
            elif classification == "suspicious":
                report.suspicious_queries.append(qa)
            else:
                report.unknown_queries.append(qa)

        # 4. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞—Å—Ö–æ–¥—É (—Å–∞–º—ã–µ –¥–æ—Ä–æ–≥–∏–µ –º—É—Å–æ—Ä–Ω—ã–µ ‚Äî –ø–µ—Ä–≤—ã–º–∏)
        report.junk_queries.sort(key=lambda x: x.cost, reverse=True)
        report.suspicious_queries.sort(key=lambda x: x.cost, reverse=True)
        report.b2b_queries.sort(key=lambda x: x.cost, reverse=True)

        # 5. –î–æ–±–∞–≤–ª—è–µ–º –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞
        if new_negatives:
            report.new_negatives_added = sorted(new_negatives)

            if auto_add:
                self._add_negatives(campaign_ids, list(new_negatives),
                                    existing_negatives)

        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        self._save_to_history(report)

        return report

    def _add_negatives(self, campaign_ids: List[int],
                       new_words: List[str],
                       existing: set):
        """–î–æ–±–∞–≤–∏—Ç—å –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞ –∫ –∫–∞–º–ø–∞–Ω–∏—è–º."""
        merged = sorted(existing | set(new_words))

        # –õ–∏–º–∏—Ç –î–∏—Ä–µ–∫—Ç–∞ ‚Äî 4096 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞ –∫–∞–º–ø–∞–Ω–∏–∏
        total_len = sum(len(w) for w in merged)
        if total_len > 4000:
            print(f"–í–ù–ò–ú–ê–ù–ò–ï: {total_len} —Å–∏–º–≤–æ–ª–æ–≤ ‚Äî –±–ª–∏–∑–∫–æ –∫ –ª–∏–º–∏—Ç—É 4096!")

        for cid in campaign_ids:
            try:
                self.api.update_campaign(cid, NegativeKeywords={"Items": merged})
                print(f"–ö–∞–º–ø–∞–Ω–∏—è {cid}: –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(new_words)} "
                      f"–º–∏–Ω—É—Å-—Å–ª–æ–≤ (–≤—Å–µ–≥–æ {len(merged)})")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∫–∞–º–ø–∞–Ω–∏–∏ {cid}: {e}")

    # ‚îÄ‚îÄ‚îÄ –ò—Å—Ç–æ—Ä–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def _load_history(self) -> dict:
        if self.HISTORY_FILE.exists():
            try:
                return json.loads(self.HISTORY_FILE.read_text(encoding="utf-8"))
            except Exception:
                return {"runs": [], "total_negatives_added": 0}
        return {"runs": [], "total_negatives_added": 0}

    def _save_to_history(self, report: AnalysisReport):
        self.HISTORY_FILE.parent.mkdir(parents=True, exist_ok=True)
        run = {
            "date": datetime.now().isoformat(),
            "period_days": report.period_days,
            "total_queries": report.total_queries,
            "junk": len(report.junk_queries),
            "b2b": len(report.b2b_queries),
            "suspicious": len(report.suspicious_queries),
            "new_negatives": report.new_negatives_added,
            "junk_cost": sum(q.cost for q in report.junk_queries),
        }
        self._history["runs"].append(run)
        self._history["total_negatives_added"] += len(report.new_negatives_added)
        self.HISTORY_FILE.write_text(
            json.dumps(self._history, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

    # ‚îÄ‚îÄ‚îÄ –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–æ–≤ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def format_report(self, report: AnalysisReport, short: bool = False) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á—ë—Ç –¥–ª—è Telegram."""
        lines = []
        lines.append(f"üìä *–ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞ {report.period_days} –¥–Ω.*\n")
        lines.append(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: *{report.total_queries}*")
        lines.append(f"–û–±—â–∏–π —Ä–∞—Å—Ö–æ–¥: *{report.total_cost:.0f} ‚ÇΩ*\n")

        # –°–≤–æ–¥–∫–∞
        junk_cost = sum(q.cost for q in report.junk_queries)
        b2b_cost = sum(q.cost for q in report.b2b_queries)
        lines.append(f"‚úÖ B2B-—Ü–µ–ª–µ–≤—ã–µ: {len(report.b2b_queries)} "
                      f"({b2b_cost:.0f} ‚ÇΩ)")
        lines.append(f"üö´ –ú—É—Å–æ—Ä–Ω—ã–µ: {len(report.junk_queries)} "
                      f"({junk_cost:.0f} ‚ÇΩ)")
        lines.append(f"‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ: {len(report.suspicious_queries)}")
        lines.append(f"‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ: {len(report.unknown_queries)}")

        if report.total_cost > 0 and junk_cost > 0:
            waste_pct = junk_cost / report.total_cost * 100
            lines.append(f"\nüí∏ *–°–ª–∏–≤ –±—é–¥–∂–µ—Ç–∞: {waste_pct:.1f}%* ({junk_cost:.0f} ‚ÇΩ)")

        if not short:
            # –¢–æ–ø –º—É—Å–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            if report.junk_queries:
                lines.append("\n*üö´ –¢–æ–ø –º—É—Å–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:*")
                for q in report.junk_queries[:10]:
                    lines.append(f"  `{q.query[:50]}` ‚Äî "
                                 f"{q.clicks} –∫–ª., {q.cost:.0f}‚ÇΩ "
                                 f"[{q.matched_pattern}]")

            # –¢–æ–ø –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö
            if report.suspicious_queries:
                lines.append("\n*‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ (–ø—Ä–æ–≤–µ—Ä—å –≤—Ä—É—á–Ω—É—é):*")
                for q in report.suspicious_queries[:10]:
                    lines.append(f"  `{q.query[:50]}` ‚Äî "
                                 f"{q.clicks} –∫–ª., {q.cost:.0f}‚ÇΩ")

            # –¢–æ–ø B2B
            if report.b2b_queries:
                lines.append("\n*‚úÖ –¢–æ–ø B2B –∑–∞–ø—Ä–æ—Å–æ–≤:*")
                for q in report.b2b_queries[:10]:
                    lines.append(f"  `{q.query[:50]}` ‚Äî "
                                 f"{q.clicks} –∫–ª., {q.cost:.0f}‚ÇΩ")

        # –ù–æ–≤—ã–µ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞
        if report.new_negatives_added:
            lines.append(f"\n*üÜï –ù–æ–≤—ã–µ –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞ ({len(report.new_negatives_added)}):*")
            lines.append(f"  `{', '.join(report.new_negatives_added[:20])}`")

        if report.existing_negatives_skipped:
            lines.append(f"‚Ü©Ô∏è –£–∂–µ –≤ –º–∏–Ω—É—Å–∞—Ö: {report.existing_negatives_skipped}")

        return "\n".join(lines)

    def format_report_plain(self, report: AnalysisReport) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏ (–±–µ–∑ Markdown)."""
        lines = []
        lines.append("=" * 60)
        lines.append(f"–ê–ù–ê–õ–ò–ó –ü–û–ò–°–ö–û–í–´–• –ó–ê–ü–†–û–°–û–í ({report.period_days} –¥–Ω.)")
        lines.append("=" * 60)
        lines.append(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {report.total_queries}")
        lines.append(f"–û–±—â–∏–π —Ä–∞—Å—Ö–æ–¥:   {report.total_cost:.2f} —Ä—É–±.")
        lines.append("")

        junk_cost = sum(q.cost for q in report.junk_queries)
        b2b_cost = sum(q.cost for q in report.b2b_queries)

        lines.append(f"B2B-—Ü–µ–ª–µ–≤—ã–µ:    {len(report.b2b_queries):>5} "
                      f"({b2b_cost:>8.2f} —Ä—É–±.)")
        lines.append(f"–ú—É—Å–æ—Ä–Ω—ã–µ:       {len(report.junk_queries):>5} "
                      f"({junk_cost:>8.2f} —Ä—É–±.)")
        lines.append(f"–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ: {len(report.suspicious_queries):>5}")
        lines.append(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ:    {len(report.unknown_queries):>5}")

        if report.total_cost > 0 and junk_cost > 0:
            lines.append(f"\n–°–ª–∏–≤ –±—é–¥–∂–µ—Ç–∞: {junk_cost / report.total_cost * 100:.1f}%"
                          f" ({junk_cost:.2f} —Ä—É–±.)")

        if report.junk_queries:
            lines.append("\n--- –ú–£–°–û–†–ù–´–ï –ó–ê–ü–†–û–°–´ (—Ç–æ–ø-15) ---")
            for q in report.junk_queries[:15]:
                lines.append(f"  {q.query[:55]:55} {q.clicks:>3}–∫–ª "
                              f"{q.cost:>7.2f}—Ä  [{q.matched_pattern}]")

        if report.suspicious_queries:
            lines.append("\n--- –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–ï (–ø—Ä–æ–≤–µ—Ä—å –≤—Ä—É—á–Ω—É—é, —Ç–æ–ø-15) ---")
            for q in report.suspicious_queries[:15]:
                lines.append(f"  {q.query[:55]:55} {q.clicks:>3}–∫–ª "
                              f"{q.cost:>7.2f}—Ä  [{q.matched_pattern}]")

        if report.b2b_queries:
            lines.append("\n--- B2B –¶–ï–õ–ï–í–´–ï (—Ç–æ–ø-15) ---")
            for q in report.b2b_queries[:15]:
                lines.append(f"  {q.query[:55]:55} {q.clicks:>3}–∫–ª "
                              f"{q.cost:>7.2f}—Ä")

        if report.unknown_queries:
            lines.append("\n--- –ù–ï–ö–õ–ê–°–°–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–´–ï (—Ç–æ–ø-15, —Ä–∞–∑–±–µ—Ä–∏ –≤—Ä—É—á–Ω—É—é) ---")
            for q in sorted(report.unknown_queries,
                            key=lambda x: x.cost, reverse=True)[:15]:
                lines.append(f"  {q.query[:55]:55} {q.clicks:>3}–∫–ª "
                              f"{q.cost:>7.2f}—Ä")

        if report.new_negatives_added:
            lines.append(f"\n–ù–û–í–´–ï –ú–ò–ù–£–°-–°–õ–û–í–ê ({len(report.new_negatives_added)}):")
            lines.append(f"  {', '.join(report.new_negatives_added)}")

        lines.append("")
        return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ CLI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def main():
    import argparse

    parser = argparse.ArgumentParser(description="–ê–Ω–∞–ª–∏–∑ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ B2B")
    parser.add_argument("--days", type=int, default=7, help="–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ (–¥–Ω–µ–π)")
    parser.add_argument("--auto-add", action="store_true",
                        help="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–∏—Ç—å –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞")
    parser.add_argument("--campaigns", type=int, nargs="+",
                        default=B2B_CAMPAIGN_IDS,
                        help="ID –∫–∞–º–ø–∞–Ω–∏–π")
    args = parser.parse_args()

    analyzer = QueryAnalyzer()
    report = analyzer.analyze(
        campaign_ids=args.campaigns,
        days=args.days,
        auto_add=args.auto_add,
    )

    print(analyzer.format_report_plain(report))

    if report.new_negatives_added and not args.auto_add:
        print("\n–î–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å --auto-add")


if __name__ == "__main__":
    main()
